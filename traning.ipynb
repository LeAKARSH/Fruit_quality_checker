{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cf41a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9c57bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your local data directory here.\n",
    "# Assuming you have folders like 'apple', 'banana', 'orange'\n",
    "# inside 'data_dir', and each of those has 'fresh' and 'rotten' subfolders.\n",
    "DATA_DIR = 'C:/Users/Akarshjayanth.M.Naik/Desktop/my_fruit_checker/data/training' # Replace with your actual path\n",
    "\n",
    "CATEGORIES = [] # List to hold combined categories (e.g., 'fresh_apple', 'rotten_banana')\n",
    "FRUIT_TYPES = [] # List to hold fruit types (apple, banana, orange) - dynamically detected\n",
    "IMG_SIZE = 50 # Image size for resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fad323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data():\n",
    "    training_data = []\n",
    "    category_index = 0 # Initialize index for combined categories\n",
    "    for fruit_type in FRUIT_TYPES: # Iterate through fruit type folders (apple, banana, etc.)\n",
    "        for quality in ['fresh', 'rotten']: # Iterate through 'fresh' and 'rotten' subfolders\n",
    "            combined_category_name = f\"{quality}_{fruit_type}\" # e.g., 'fresh_apple'\n",
    "            CATEGORIES.append(combined_category_name) # Add combined category to list\n",
    "            path = os.path.join(DATA_DIR, fruit_type, quality) # Path to fruit_type/quality folder\n",
    "            class_num = category_index # Use current category_index as class number\n",
    "            for img in os.listdir(path): # Iterate through images in the folder\n",
    "                try:\n",
    "                    img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE) # Load in grayscale\n",
    "                    if img_array is None: # Check if image was loaded successfully\n",
    "                        continue\n",
    "                    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE)) # Resize to IMG_SIZE x IMG_SIZE\n",
    "                    training_data.append([new_array, class_num]) # Add image and label\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image: {img} in category: {combined_category_name}\")\n",
    "                    print(e)\n",
    "                    pass\n",
    "            category_index += 1 # Increment category index for the next combined category\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbd15bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Fruit Types: ['apple', 'banana', 'orange']\n",
      "Combined Categories (for model output): ['fresh_apple', 'rotten_apple', 'fresh_banana', 'rotten_banana', 'fresh_orange', 'rotten_orange']\n",
      "Number of Combined Categories: 6\n",
      "Shape of X: (14133, 50, 50, 1)\n",
      "Shape of y: (14133,)\n"
     ]
    }
   ],
   "source": [
    "# Dynamically detect fruit types (subfolders in DATA_DIR)\n",
    "for item in os.listdir(DATA_DIR):\n",
    "    fruit_type_path = os.path.join(DATA_DIR, item)\n",
    "    if os.path.isdir(fruit_type_path):\n",
    "        # Check if it contains 'fresh' and 'rotten' subfolders (basic validation)\n",
    "        if os.path.isdir(os.path.join(fruit_type_path, 'fresh')) and os.path.isdir(os.path.join(fruit_type_path, 'rotten')):\n",
    "            FRUIT_TYPES.append(item)\n",
    "\n",
    "print(\"Detected Fruit Types:\", FRUIT_TYPES)\n",
    "\n",
    "if not FRUIT_TYPES:\n",
    "    raise ValueError(f\"No fruit type folders with 'fresh' and 'rotten' subfolders found in '{DATA_DIR}'. Please check your data directory structure.\")\n",
    "\n",
    "training_data = create_training_data()\n",
    "\n",
    "print(\"Combined Categories (for model output):\", CATEGORIES)\n",
    "print(\"Number of Combined Categories:\", len(CATEGORIES))\n",
    "\n",
    "random.shuffle(training_data)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y = np.array(y)\n",
    "\n",
    "X = X/255.0\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d1fdd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akarshjayanth.M.Naik\\Desktop\\my_fruit_checker\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4893 - loss: 1.2782 - val_accuracy: 0.7380 - val_loss: 0.7246\n",
      "Epoch 2/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 30ms/step - accuracy: 0.7216 - loss: 0.7407 - val_accuracy: 0.7285 - val_loss: 0.6583\n",
      "Epoch 3/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 37ms/step - accuracy: 0.7864 - loss: 0.5659 - val_accuracy: 0.8394 - val_loss: 0.4530\n",
      "Epoch 4/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - accuracy: 0.8228 - loss: 0.4777 - val_accuracy: 0.8017 - val_loss: 0.5281\n",
      "Epoch 5/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - accuracy: 0.8401 - loss: 0.4246 - val_accuracy: 0.8677 - val_loss: 0.3570\n",
      "Epoch 6/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 36ms/step - accuracy: 0.8734 - loss: 0.3371 - val_accuracy: 0.8795 - val_loss: 0.3424\n",
      "Epoch 7/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.8893 - loss: 0.3083 - val_accuracy: 0.8660 - val_loss: 0.3492\n",
      "Epoch 8/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - accuracy: 0.8916 - loss: 0.3011 - val_accuracy: 0.9116 - val_loss: 0.2485\n",
      "Epoch 9/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - accuracy: 0.9065 - loss: 0.2509 - val_accuracy: 0.8908 - val_loss: 0.2903\n",
      "Epoch 10/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 0.8996 - loss: 0.2680 - val_accuracy: 0.9243 - val_loss: 0.2199\n",
      "Epoch 11/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 0.9221 - loss: 0.2128 - val_accuracy: 0.9200 - val_loss: 0.2219\n",
      "Epoch 12/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - accuracy: 0.9370 - loss: 0.1814 - val_accuracy: 0.9132 - val_loss: 0.2486\n",
      "Epoch 13/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 0.9327 - loss: 0.1881 - val_accuracy: 0.9267 - val_loss: 0.2079\n",
      "Epoch 14/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 36ms/step - accuracy: 0.9389 - loss: 0.1631 - val_accuracy: 0.9248 - val_loss: 0.2082\n",
      "Epoch 15/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - accuracy: 0.9345 - loss: 0.1744 - val_accuracy: 0.9229 - val_loss: 0.2171\n",
      "Epoch 16/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - accuracy: 0.9484 - loss: 0.1553 - val_accuracy: 0.9351 - val_loss: 0.1689\n",
      "Epoch 17/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 93ms/step - accuracy: 0.9545 - loss: 0.1271 - val_accuracy: 0.9380 - val_loss: 0.1835\n",
      "Epoch 18/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 96ms/step - accuracy: 0.9519 - loss: 0.1324 - val_accuracy: 0.9283 - val_loss: 0.2054\n",
      "Epoch 19/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.9502 - loss: 0.1331 - val_accuracy: 0.9453 - val_loss: 0.1492\n",
      "Epoch 20/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 94ms/step - accuracy: 0.9584 - loss: 0.1132 - val_accuracy: 0.9347 - val_loss: 0.1767\n",
      "Epoch 21/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 67ms/step - accuracy: 0.9516 - loss: 0.1329 - val_accuracy: 0.9387 - val_loss: 0.1772\n",
      "Epoch 22/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 43ms/step - accuracy: 0.9508 - loss: 0.1310 - val_accuracy: 0.9344 - val_loss: 0.1887\n",
      "Epoch 23/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - accuracy: 0.9637 - loss: 0.1015 - val_accuracy: 0.9540 - val_loss: 0.1437\n",
      "Epoch 24/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - accuracy: 0.9626 - loss: 0.1017 - val_accuracy: 0.9500 - val_loss: 0.1529\n",
      "Epoch 25/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 37ms/step - accuracy: 0.9698 - loss: 0.0811 - val_accuracy: 0.9552 - val_loss: 0.1359\n",
      "Epoch 26/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 98ms/step - accuracy: 0.9666 - loss: 0.0917 - val_accuracy: 0.9514 - val_loss: 0.1638\n",
      "Epoch 27/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9626 - loss: 0.1057 - val_accuracy: 0.9465 - val_loss: 0.1698\n",
      "Epoch 28/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 89ms/step - accuracy: 0.9740 - loss: 0.0776 - val_accuracy: 0.9533 - val_loss: 0.1461\n",
      "Epoch 29/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 96ms/step - accuracy: 0.9691 - loss: 0.0873 - val_accuracy: 0.9526 - val_loss: 0.1558\n",
      "Epoch 30/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 56ms/step - accuracy: 0.9653 - loss: 0.0963 - val_accuracy: 0.9427 - val_loss: 0.1724\n",
      "Epoch 31/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 69ms/step - accuracy: 0.9674 - loss: 0.0923 - val_accuracy: 0.9514 - val_loss: 0.1500\n",
      "Epoch 32/32\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 99ms/step - accuracy: 0.9695 - loss: 0.0864 - val_accuracy: 0.9540 - val_loss: 0.1520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as fruitquality-structured-cnn (h5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=X.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Output layer now matches the number of combined categories\n",
    "model.add(Dense(len(CATEGORIES))) # Output layer size is number of combined categories\n",
    "model.add(Activation('softmax')) # Softmax for multi-class\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', # Sparse categorical crossentropy for integer labels\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, batch_size=32, epochs=32, validation_split=0.3)\n",
    "\n",
    "model.save('fruitquality-structured-cnn.h5')\n",
    "print(\"Model saved as fruitquality-structured-cnn (h5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a8e7da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
